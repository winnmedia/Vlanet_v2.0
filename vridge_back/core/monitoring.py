"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ë„êµ¬
"""

import time
import logging
import functools
from datetime import datetime
from django.conf import settings
from django.core.cache import cache
from django.db import connection
from django.utils.decorators import method_decorator


logger = logging.getLogger('performance')


class PerformanceMonitor:
    """
    ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.metrics = {
            'db_queries': [],
            'cache_hits': 0,
            'cache_misses': 0,
            'slow_requests': [],
            'api_calls': {},
        }
    
    def log_db_query(self, query, duration):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë¡œê¹…"""
        if duration > 0.1:  # 100ms ì´ìƒ
            self.metrics['db_queries'].append({
                'query': query,
                'duration': duration,
                'timestamp': datetime.now().isoformat()
            })
    
    def log_cache_hit(self):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        self.metrics['cache_hits'] += 1
    
    def log_cache_miss(self):
        """ìºì‹œ ë¯¸ìŠ¤ ê¸°ë¡"""
        self.metrics['cache_misses'] += 1
    
    def log_slow_request(self, path, duration):
        """ëŠë¦° ìš”ì²­ ê¸°ë¡"""
        if duration > 1.0:  # 1ì´ˆ ì´ìƒ
            self.metrics['slow_requests'].append({
                'path': path,
                'duration': duration,
                'timestamp': datetime.now().isoformat()
            })
    
    def get_metrics(self):
        """ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.metrics.copy()
    
    def reset_metrics(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.metrics = {
            'db_queries': [],
            'cache_hits': 0,
            'cache_misses': 0,
            'slow_requests': [],
            'api_calls': {},
        }


# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()


# ë°ì½”ë ˆì´í„°ë“¤

def measure_time(func):
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        duration = end_time - start_time
        logger.info(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {duration:.3f}ì´ˆ")
        
        return result
    return wrapper


def cache_result(timeout=300, key_prefix=''):
    """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"{key_prefix}:{func.__name__}:{str(args)}:{str(kwargs)}"
            
            # ìºì‹œ í™•ì¸
            cached = cache.get(cache_key)
            if cached is not None:
                monitor.log_cache_hit()
                return cached
            
            # ìºì‹œ ë¯¸ìŠ¤
            monitor.log_cache_miss()
            result = func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            cache.set(cache_key, result, timeout)
            
            return result
        return wrapper
    return decorator


def optimize_db_queries(func):
    """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # ì¿¼ë¦¬ ì¹´ìš´íŠ¸ ì‹œì‘
        initial_queries = len(connection.queries)
        
        result = func(*args, **kwargs)
        
        # ì‹¤í–‰ëœ ì¿¼ë¦¬ ìˆ˜
        query_count = len(connection.queries) - initial_queries
        
        if query_count > 10:
            logger.warning(f"{func.__name__}ì—ì„œ {query_count}ê°œì˜ ì¿¼ë¦¬ ì‹¤í–‰ë¨ (ìµœì í™” í•„ìš”)")
        
        return result
    return wrapper


# ë¯¸ë“¤ì›¨ì–´

class PerformanceMiddleware:
    """
    ìš”ì²­ ì„±ëŠ¥ ì¸¡ì • ë¯¸ë“¤ì›¨ì–´
    """
    
    def __init__(self, get_response):
        self.get_response = get_response
    
    def __call__(self, request):
        # ìš”ì²­ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        
        # ì¿¼ë¦¬ ì¹´ìš´íŠ¸ ì‹œì‘
        initial_queries = len(connection.queries)
        
        response = self.get_response(request)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        duration = time.time() - start_time
        
        # ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
        query_count = len(connection.queries) - initial_queries
        
        # ëŠë¦° ìš”ì²­ ë¡œê¹…
        if duration > 1.0:
            monitor.log_slow_request(request.path, duration)
            logger.warning(f"ëŠë¦° ìš”ì²­: {request.path} ({duration:.2f}ì´ˆ, {query_count}ê°œ ì¿¼ë¦¬)")
        
        # ì‘ë‹µ í—¤ë”ì— ì„±ëŠ¥ ì •ë³´ ì¶”ê°€ (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
        if settings.DEBUG:
            response['X-Response-Time'] = f"{duration:.3f}"
            response['X-DB-Query-Count'] = str(query_count)
        
        return response


# ì¿¼ë¦¬ ìµœì í™” ë„êµ¬

def analyze_queries():
    """
    ì‹¤í–‰ëœ ì¿¼ë¦¬ ë¶„ì„
    """
    from django.db import connection
    from collections import defaultdict
    
    query_stats = defaultdict(lambda: {'count': 0, 'total_time': 0})
    
    for query in connection.queries:
        sql = query['sql']
        time = float(query.get('time', 0))
        
        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_name = 'unknown'
        if 'FROM' in sql:
            parts = sql.split('FROM')[1].split()
            if parts:
                table_name = parts[0].strip('"').strip('`')
        
        query_stats[table_name]['count'] += 1
        query_stats[table_name]['total_time'] += time
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼:")
    print("-" * 80)
    print(f"{'í…Œì´ë¸”':<30} {'ì¿¼ë¦¬ ìˆ˜':<10} {'ì´ ì‹œê°„(ì´ˆ)':<15} {'í‰ê·  ì‹œê°„(ì´ˆ)':<15}")
    print("-" * 80)
    
    for table, stats in sorted(query_stats.items(), 
                               key=lambda x: x[1]['total_time'], 
                               reverse=True):
        avg_time = stats['total_time'] / stats['count'] if stats['count'] > 0 else 0
        print(f"{table:<30} {stats['count']:<10} "
              f"{stats['total_time']:<15.3f} {avg_time:<15.3f}")
    
    total_queries = sum(s['count'] for s in query_stats.values())
    total_time = sum(s['total_time'] for s in query_stats.values())
    
    print("-" * 80)
    print(f"{'ì´ê³„':<30} {total_queries:<10} {total_time:<15.3f}")
    print("\nğŸ’¡ ìµœì í™” ì œì•ˆ:")
    
    # ìµœì í™” ì œì•ˆ
    for table, stats in query_stats.items():
        if stats['count'] > 10:
            print(f"  - {table}: select_related() ë˜ëŠ” prefetch_related() ì‚¬ìš© ê³ ë ¤")
        if stats['total_time'] > 0.5:
            print(f"  - {table}: ì¸ë±ìŠ¤ ì¶”ê°€ ê³ ë ¤")


# ìºì‹œ ì›Œë°

def warm_cache():
    """
    ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ìºì‹±
    """
    from projects.models import Project
    from users.models import User
    
    print("ğŸ”¥ ìºì‹œ ì›Œë° ì‹œì‘...")
    
    # í™œì„± í”„ë¡œì íŠ¸ ìºì‹±
    active_projects = Project.objects.filter(
        is_active=True
    ).select_related('user', 'feedback').prefetch_related('member_list')
    
    for project in active_projects:
        cache_key = f"project:{project.id}"
        cache.set(cache_key, project, 3600)
    
    print(f"âœ… {active_projects.count()}ê°œ í”„ë¡œì íŠ¸ ìºì‹± ì™„ë£Œ")
    
    # í™œì„± ì‚¬ìš©ì ìºì‹±
    active_users = User.objects.filter(
        is_active=True,
        last_login__isnull=False
    )
    
    for user in active_users:
        cache_key = f"user:{user.id}"
        cache.set(cache_key, user, 3600)
    
    print(f"âœ… {active_users.count()}ëª… ì‚¬ìš©ì ìºì‹± ì™„ë£Œ")


# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

def load_test(url, num_requests=100, num_concurrent=10):
    """
    ê°„ë‹¨í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
    """
    import concurrent.futures
    import requests
    
    print(f"ğŸš€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {url}")
    print(f"   ìš”ì²­ ìˆ˜: {num_requests}")
    print(f"   ë™ì‹œ ì—°ê²°: {num_concurrent}")
    
    def make_request():
        start = time.time()
        try:
            response = requests.get(url, timeout=30)
            duration = time.time() - start
            return {
                'status': response.status_code,
                'duration': duration,
                'success': response.status_code == 200
            }
        except Exception as e:
            return {
                'status': 0,
                'duration': time.time() - start,
                'success': False,
                'error': str(e)
            }
    
    # ë™ì‹œ ì‹¤í–‰
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:
        futures = [executor.submit(make_request) for _ in range(num_requests)]
        
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())
    
    # ê²°ê³¼ ë¶„ì„
    successful = sum(1 for r in results if r['success'])
    failed = num_requests - successful
    durations = [r['duration'] for r in results if r['success']]
    
    if durations:
        avg_duration = sum(durations) / len(durations)
        min_duration = min(durations)
        max_duration = max(durations)
    else:
        avg_duration = min_duration = max_duration = 0
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì„±ê³µ: {successful}/{num_requests} ({successful/num_requests*100:.1f}%)")
    print(f"   ì‹¤íŒ¨: {failed}")
    print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_duration:.3f}ì´ˆ")
    print(f"   ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_duration:.3f}ì´ˆ")
    print(f"   ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_duration:.3f}ì´ˆ")
    print(f"   ì²˜ë¦¬ëŸ‰: {successful/sum(durations):.1f} req/sec" if durations else "")


# ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ì œì•ˆ

def suggest_indexes():
    """
    ì¸ë±ìŠ¤ ì¶”ê°€ ì œì•ˆ
    """
    suggestions = []
    
    # ìì£¼ ì¡°íšŒë˜ëŠ” í•„ë“œ í™•ì¸
    common_lookups = {
        'projects.Project': ['user', 'created', 'is_active'],
        'feedbacks.FeedBack': ['project', 'created'],
        'feedbacks.FeedBackItem': ['feedback', 'user', 'created'],
        'users.User': ['email', 'username', 'is_active'],
    }
    
    for model_path, fields in common_lookups.items():
        app_label, model_name = model_path.split('.')
        
        for field in fields:
            suggestions.append({
                'model': model_name,
                'field': field,
                'sql': f"CREATE INDEX idx_{model_name.lower()}_{field} "
                      f"ON {app_label}_{model_name.lower()} ({field});"
            })
    
    print("\nğŸ” ì¸ë±ìŠ¤ ì¶”ê°€ ì œì•ˆ:")
    for suggestion in suggestions:
        print(f"\nëª¨ë¸: {suggestion['model']}")
        print(f"í•„ë“œ: {suggestion['field']}")
        print(f"SQL: {suggestion['sql']}")
    
    return suggestions


# Sentry í†µí•© (í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§)

def setup_sentry():
    """
    Sentry ì„¤ì •
    """
    import sentry_sdk
    from sentry_sdk.integrations.django import DjangoIntegration
    from sentry_sdk.integrations.redis import RedisIntegration
    
    if settings.SENTRY_DSN:
        sentry_sdk.init(
            dsn=settings.SENTRY_DSN,
            integrations=[
                DjangoIntegration(),
                RedisIntegration(),
            ],
            traces_sample_rate=0.1,  # 10% ìƒ˜í”Œë§
            send_default_pii=False,  # ê°œì¸ì •ë³´ ì „ì†¡ ì•ˆí•¨
            environment=settings.ENVIRONMENT,
            release=settings.VERSION,
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            profiles_sample_rate=0.1,
            
            # í•„í„°ë§
            before_send=lambda event, hint: event if not settings.DEBUG else None,
            
            # ë¬´ì‹œí•  ì—ëŸ¬
            ignore_errors=[
                'KeyboardInterrupt',
                'SystemExit',
                'Http404',
            ],
        )
        
        logger.info("Sentry ëª¨ë‹ˆí„°ë§ í™œì„±í™”ë¨")