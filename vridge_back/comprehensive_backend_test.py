#!/usr/bin/env python3
"""
VideoPlanet ë°±ì—”ë“œ ì¢…í•© ê²€ì¦ ë° ê°œì„  ìŠ¤í¬ë¦½íŠ¸
ë°±ì—”ë“œ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±, ì„±ëŠ¥, ë³´ì•ˆì„ ì²´ê³„ì ìœ¼ë¡œ ê²€ì¦í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import requests
import psycopg2
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging
from urllib.parse import urlparse

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# Django ì„¤ì •
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings_base')
import django
django.setup()

from django.core.management import call_command
from django.db import connection
from django.test.utils import override_settings
from django.core.cache import cache
from django.contrib.auth import get_user_model

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('backend_test_report.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

User = get_user_model()

class BackendSystemValidator:
    """ë°±ì—”ë“œ ì‹œìŠ¤í…œ ê²€ì¦ ë° ê°œì„  í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = os.environ.get('API_URL', 'http://localhost:8000')
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'server_status': {},
            'database': {},
            'api_endpoints': {},
            'security': {},
            'performance': {},
            'improvements': []
        }
        
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("="*60)
        logger.info("VideoPlanet ë°±ì—”ë“œ ì‹œìŠ¤í…œ ê²€ì¦ ì‹œì‘")
        logger.info("="*60)
        
        # 1. Django ì„œë²„ ìƒíƒœ í™•ì¸
        self.check_django_server()
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ìƒíƒœ í™•ì¸
        self.check_database_connection()
        
        # 3. ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ ê²€ì¦
        self.check_migrations()
        
        # 4. API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
        self.test_api_endpoints()
        
        # 5. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë¶„ì„
        self.analyze_database_optimization()
        
        # 6. ë³´ì•ˆ ì„¤ì • ê²€ì¦
        self.check_security_settings()
        
        # 7. ì„±ëŠ¥ ë¶„ì„
        self.analyze_performance()
        
        # 8. Redis ìºì‹œ ìƒíƒœ í™•ì¸
        self.check_redis_cache()
        
        # 9. ì—ëŸ¬ ë¡œê¹… ì‹œìŠ¤í…œ ê²€ì¦
        self.check_error_logging()
        
        # 10. ê°œì„ ì‚¬í•­ ë„ì¶œ
        self.generate_improvements()
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
        
    def check_django_server(self):
        """Django ì„œë²„ ìƒíƒœ í™•ì¸"""
        logger.info("\n[1] Django ì„œë²„ ìƒíƒœ í™•ì¸")
        
        try:
            # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
            response = requests.get(f"{self.base_url}/api/health/", timeout=5)
            
            if response.status_code == 200:
                self.test_results['server_status'] = {
                    'status': 'running',
                    'response_time': response.elapsed.total_seconds(),
                    'details': response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
                }
                logger.info(f"âœ… ì„œë²„ ì •ìƒ ì‘ë™ (ì‘ë‹µì‹œê°„: {response.elapsed.total_seconds():.2f}ì´ˆ)")
            else:
                self.test_results['server_status'] = {
                    'status': 'error',
                    'status_code': response.status_code
                }
                logger.warning(f"âš ï¸ ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
                
        except requests.exceptions.ConnectionError:
            self.test_results['server_status'] = {'status': 'not_running'}
            logger.error("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    def check_database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
        logger.info("\n[2] ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸")
        
        try:
            with connection.cursor() as cursor:
                # PostgreSQL ë²„ì „ í™•ì¸
                cursor.execute("SELECT version();")
                db_version = cursor.fetchone()[0]
                
                # í…Œì´ë¸” ìˆ˜ í™•ì¸
                cursor.execute("""
                    SELECT COUNT(*) 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public';
                """)
                table_count = cursor.fetchone()[0]
                
                # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
                cursor.execute("""
                    SELECT pg_database_size(current_database()) / 1024 / 1024 as size_mb;
                """)
                db_size = cursor.fetchone()[0]
                
                self.test_results['database'] = {
                    'status': 'connected',
                    'version': db_version,
                    'table_count': table_count,
                    'size_mb': float(db_size)
                }
                
                logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
                logger.info(f"   - ë²„ì „: {db_version.split(',')[0]}")
                logger.info(f"   - í…Œì´ë¸” ìˆ˜: {table_count}")
                logger.info(f"   - í¬ê¸°: {db_size:.2f} MB")
                
        except Exception as e:
            self.test_results['database']['status'] = 'error'
            self.test_results['database']['error'] = str(e)
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            
    def check_migrations(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸"""
        logger.info("\n[3] ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸")
        
        try:
            from django.core.management import call_command
            from io import StringIO
            
            out = StringIO()
            call_command('showmigrations', '--list', stdout=out)
            migrations_output = out.getvalue()
            
            # ì ìš©ë˜ì§€ ì•Šì€ ë§ˆì´ê·¸ë ˆì´ì…˜ í™•ì¸
            unapplied = []
            for line in migrations_output.split('\n'):
                if '[ ]' in line:
                    unapplied.append(line.strip())
                    
            self.test_results['database']['migrations'] = {
                'status': 'checked',
                'unapplied_count': len(unapplied),
                'unapplied': unapplied[:10]  # ì²˜ìŒ 10ê°œë§Œ
            }
            
            if unapplied:
                logger.warning(f"âš ï¸ ì ìš©ë˜ì§€ ì•Šì€ ë§ˆì´ê·¸ë ˆì´ì…˜ {len(unapplied)}ê°œ ë°œê²¬")
                for migration in unapplied[:5]:
                    logger.warning(f"   - {migration}")
            else:
                logger.info("âœ… ëª¨ë“  ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš© ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ í™•ì¸ ì‹¤íŒ¨: {e}")
            
    def test_api_endpoints(self):
        """API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        logger.info("\n[4] API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸")
        
        endpoints = [
            {'path': '/api/auth/login/', 'method': 'POST', 'data': {'email': 'test@test.com', 'password': 'test123'}},
            {'path': '/api/auth/check/', 'method': 'GET'},
            {'path': '/api/projects/', 'method': 'GET'},
            {'path': '/api/feedbacks/', 'method': 'GET'},
            {'path': '/api/video-planning/', 'method': 'GET'},
            {'path': '/api/users/mypage/', 'method': 'GET'},
        ]
        
        for endpoint in endpoints:
            path = endpoint['path']
            method = endpoint['method']
            
            try:
                if method == 'GET':
                    response = requests.get(f"{self.base_url}{path}", timeout=5)
                else:
                    response = requests.post(
                        f"{self.base_url}{path}",
                        json=endpoint.get('data', {}),
                        timeout=5
                    )
                    
                self.test_results['api_endpoints'][path] = {
                    'status_code': response.status_code,
                    'response_time': response.elapsed.total_seconds(),
                    'success': response.status_code in [200, 201, 401, 403]
                }
                
                status_icon = "âœ…" if response.status_code in [200, 201] else "âš ï¸"
                logger.info(f"{status_icon} {method} {path}: {response.status_code} ({response.elapsed.total_seconds():.2f}ì´ˆ)")
                
            except Exception as e:
                self.test_results['api_endpoints'][path] = {
                    'error': str(e),
                    'success': False
                }
                logger.error(f"âŒ {method} {path}: {e}")
                
    def analyze_database_optimization(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë¶„ì„"""
        logger.info("\n[5] ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë¶„ì„")
        
        optimization_issues = []
        
        try:
            with connection.cursor() as cursor:
                # ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
                cursor.execute("""
                    SELECT 
                        schemaname,
                        tablename,
                        indexname,
                        idx_scan,
                        idx_tup_read,
                        idx_tup_fetch
                    FROM pg_stat_user_indexes
                    WHERE idx_scan = 0
                    AND indexname NOT LIKE '%_pkey'
                    ORDER BY schemaname, tablename;
                """)
                
                unused_indexes = cursor.fetchall()
                if unused_indexes:
                    optimization_issues.append({
                        'type': 'unused_indexes',
                        'count': len(unused_indexes),
                        'details': [f"{idx[1]}.{idx[2]}" for idx in unused_indexes[:5]]
                    })
                    logger.warning(f"âš ï¸ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ {len(unused_indexes)}ê°œ ë°œê²¬")
                    
                # ëŠë¦° ì¿¼ë¦¬ í™•ì¸ (pg_stat_statementsê°€ ìˆëŠ” ê²½ìš°)
                try:
                    cursor.execute("""
                        SELECT COUNT(*) FROM pg_extension WHERE extname = 'pg_stat_statements';
                    """)
                    if cursor.fetchone()[0] > 0:
                        cursor.execute("""
                            SELECT 
                                calls,
                                mean_exec_time,
                                query
                            FROM pg_stat_statements
                            WHERE mean_exec_time > 100
                            ORDER BY mean_exec_time DESC
                            LIMIT 5;
                        """)
                        slow_queries = cursor.fetchall()
                        if slow_queries:
                            optimization_issues.append({
                                'type': 'slow_queries',
                                'count': len(slow_queries),
                                'details': [f"í‰ê·  {q[1]:.2f}ms" for q in slow_queries]
                            })
                            logger.warning(f"âš ï¸ ëŠë¦° ì¿¼ë¦¬ {len(slow_queries)}ê°œ ë°œê²¬")
                except:
                    pass
                    
                # í…Œì´ë¸” í¬ê¸°ì™€ vacuum ìƒíƒœ í™•ì¸
                cursor.execute("""
                    SELECT 
                        schemaname,
                        tablename,
                        n_dead_tup,
                        last_vacuum,
                        last_autovacuum
                    FROM pg_stat_user_tables
                    WHERE n_dead_tup > 1000
                    ORDER BY n_dead_tup DESC
                    LIMIT 5;
                """)
                
                vacuum_needed = cursor.fetchall()
                if vacuum_needed:
                    optimization_issues.append({
                        'type': 'vacuum_needed',
                        'count': len(vacuum_needed),
                        'details': [f"{t[1]} ({t[2]} dead tuples)" for t in vacuum_needed]
                    })
                    logger.warning(f"âš ï¸ VACUUMì´ í•„ìš”í•œ í…Œì´ë¸” {len(vacuum_needed)}ê°œ")
                    
            self.test_results['performance']['db_optimization'] = optimization_issues
            
            if not optimization_issues:
                logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ìƒíƒœ ì–‘í˜¸")
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            
    def check_security_settings(self):
        """ë³´ì•ˆ ì„¤ì • ê²€ì¦"""
        logger.info("\n[6] ë³´ì•ˆ ì„¤ì • ê²€ì¦")
        
        security_checks = {
            'cors_configured': False,
            'csrf_protection': False,
            'secure_cookies': False,
            'xss_protection': False,
            'rate_limiting': False,
            'jwt_configured': False
        }
        
        try:
            from django.conf import settings
            
            # CORS ì„¤ì • í™•ì¸
            if hasattr(settings, 'CORS_ALLOWED_ORIGINS'):
                security_checks['cors_configured'] = bool(settings.CORS_ALLOWED_ORIGINS)
                logger.info(f"âœ… CORS ì„¤ì •: {len(settings.CORS_ALLOWED_ORIGINS)}ê°œ ë„ë©”ì¸ í—ˆìš©")
            else:
                logger.warning("âš ï¸ CORS ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤")
                
            # CSRF ë³´í˜¸ í™•ì¸
            if 'django.middleware.csrf.CsrfViewMiddleware' in settings.MIDDLEWARE:
                security_checks['csrf_protection'] = True
                logger.info("âœ… CSRF ë³´í˜¸ í™œì„±í™”")
            else:
                logger.warning("âš ï¸ CSRF ë³´í˜¸ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                
            # Secure Cookies í™•ì¸
            if hasattr(settings, 'SESSION_COOKIE_SECURE'):
                security_checks['secure_cookies'] = settings.SESSION_COOKIE_SECURE
                if settings.SESSION_COOKIE_SECURE:
                    logger.info("âœ… Secure Cookie ì„¤ì • í™œì„±í™”")
                else:
                    logger.warning("âš ï¸ Secure Cookieê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                    
            # XSS ë³´í˜¸ í™•ì¸
            if hasattr(settings, 'SECURE_BROWSER_XSS_FILTER'):
                security_checks['xss_protection'] = settings.SECURE_BROWSER_XSS_FILTER
                if settings.SECURE_BROWSER_XSS_FILTER:
                    logger.info("âœ… XSS í•„í„° í™œì„±í™”")
                    
            # Rate Limiting í™•ì¸
            if 'config.rate_limit_middleware.RateLimitMiddleware' in settings.MIDDLEWARE:
                security_checks['rate_limiting'] = True
                logger.info("âœ… Rate Limiting í™œì„±í™”")
            else:
                logger.warning("âš ï¸ Rate Limitingì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
            # JWT ì„¤ì • í™•ì¸
            if hasattr(settings, 'SIMPLE_JWT'):
                security_checks['jwt_configured'] = True
                logger.info("âœ… JWT ì¸ì¦ ì„¤ì • ì™„ë£Œ")
                
            self.test_results['security'] = security_checks
            
        except Exception as e:
            logger.error(f"âŒ ë³´ì•ˆ ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")
            
    def analyze_performance(self):
        """ì„±ëŠ¥ ë¶„ì„"""
        logger.info("\n[7] ì„±ëŠ¥ ë¶„ì„")
        
        performance_metrics = {}
        
        try:
            # ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸
            endpoints_to_test = [
                '/api/health/',
                '/api/projects/',
                '/api/feedbacks/'
            ]
            
            for endpoint in endpoints_to_test:
                times = []
                for _ in range(3):
                    try:
                        start = time.time()
                        response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                        elapsed = time.time() - start
                        times.append(elapsed)
                    except:
                        pass
                        
                if times:
                    avg_time = sum(times) / len(times)
                    performance_metrics[endpoint] = {
                        'avg_response_time': avg_time,
                        'min_time': min(times),
                        'max_time': max(times)
                    }
                    
                    status = "âœ…" if avg_time < 1.0 else "âš ï¸"
                    logger.info(f"{status} {endpoint}: í‰ê·  {avg_time:.3f}ì´ˆ")
                    
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ìƒíƒœ
            with connection.cursor() as cursor:
                cursor.execute("""
                    SELECT count(*) FROM pg_stat_activity;
                """)
                active_connections = cursor.fetchone()[0]
                
                cursor.execute("""
                    SELECT setting FROM pg_settings WHERE name = 'max_connections';
                """)
                max_connections = int(cursor.fetchone()[0])
                
                performance_metrics['db_connections'] = {
                    'active': active_connections,
                    'max': max_connections,
                    'usage_percent': (active_connections / max_connections) * 100
                }
                
                logger.info(f"âœ… DB ì—°ê²°: {active_connections}/{max_connections} ({(active_connections/max_connections)*100:.1f}% ì‚¬ìš©)")
                
            self.test_results['performance']['metrics'] = performance_metrics
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
    def check_redis_cache(self):
        """Redis ìºì‹œ ìƒíƒœ í™•ì¸"""
        logger.info("\n[8] Redis ìºì‹œ ìƒíƒœ í™•ì¸")
        
        if not REDIS_AVAILABLE:
            self.test_results['performance']['redis'] = {'connected': False, 'error': 'redis module not installed'}
            logger.warning("âš ï¸ Redis ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
            
        try:
            redis_url = os.environ.get('REDIS_URL', 'redis://localhost:6379')
            parsed = urlparse(redis_url)
            
            r = redis.Redis(
                host=parsed.hostname or 'localhost',
                port=parsed.port or 6379,
                password=parsed.password,
                decode_responses=True
            )
            
            # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
            r.ping()
            
            # Redis ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            info = r.info()
            
            cache_stats = {
                'connected': True,
                'version': info.get('redis_version'),
                'used_memory_mb': info.get('used_memory', 0) / 1024 / 1024,
                'connected_clients': info.get('connected_clients'),
                'total_commands': info.get('total_commands_processed'),
                'keyspace_hits': info.get('keyspace_hits', 0),
                'keyspace_misses': info.get('keyspace_misses', 0)
            }
            
            # íˆíŠ¸ìœ¨ ê³„ì‚°
            if cache_stats['keyspace_hits'] + cache_stats['keyspace_misses'] > 0:
                hit_rate = cache_stats['keyspace_hits'] / (cache_stats['keyspace_hits'] + cache_stats['keyspace_misses']) * 100
                cache_stats['hit_rate'] = hit_rate
                logger.info(f"âœ… Redis ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1f}%")
                
            self.test_results['performance']['redis'] = cache_stats
            logger.info(f"âœ… Redis ì—°ê²° ì„±ê³µ (v{cache_stats['version']})")
            logger.info(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©: {cache_stats['used_memory_mb']:.2f} MB")
            logger.info(f"   - ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸: {cache_stats['connected_clients']}")
            
        except:
            self.test_results['performance']['redis'] = {'connected': False}
            logger.warning("âš ï¸ Redisì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    def check_error_logging(self):
        """ì—ëŸ¬ ë¡œê¹… ì‹œìŠ¤í…œ ê²€ì¦"""
        logger.info("\n[9] ì—ëŸ¬ ë¡œê¹… ì‹œìŠ¤í…œ ê²€ì¦")
        
        logging_checks = {
            'log_files_exist': False,
            'error_tracking': False,
            'log_rotation': False
        }
        
        try:
            # ë¡œê·¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            log_dir = os.path.join(os.path.dirname(__file__), 'logs')
            if os.path.exists(log_dir):
                log_files = os.listdir(log_dir)
                logging_checks['log_files_exist'] = len(log_files) > 0
                
                if log_files:
                    logger.info(f"âœ… ë¡œê·¸ íŒŒì¼ {len(log_files)}ê°œ ë°œê²¬")
                    for log_file in log_files[:3]:
                        file_path = os.path.join(log_dir, log_file)
                        size_mb = os.path.getsize(file_path) / 1024 / 1024
                        logger.info(f"   - {log_file}: {size_mb:.2f} MB")
                        
            # Django ë¡œê¹… ì„¤ì • í™•ì¸
            from django.conf import settings
            if hasattr(settings, 'LOGGING'):
                logging_config = settings.LOGGING
                if 'handlers' in logging_config:
                    logging_checks['error_tracking'] = True
                    logger.info(f"âœ… Django ë¡œê¹… ì„¤ì •: {len(logging_config['handlers'])}ê°œ í•¸ë“¤ëŸ¬")
                    
            self.test_results['logging'] = logging_checks
            
        except Exception as e:
            logger.error(f"âŒ ë¡œê¹… ì‹œìŠ¤í…œ í™•ì¸ ì‹¤íŒ¨: {e}")
            
    def generate_improvements(self):
        """ê°œì„ ì‚¬í•­ ë„ì¶œ"""
        logger.info("\n[10] ê°œì„ ì‚¬í•­ ë„ì¶œ")
        
        improvements = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” í•„ìš”
        if 'db_optimization' in self.test_results.get('performance', {}):
            for issue in self.test_results['performance']['db_optimization']:
                if issue['type'] == 'unused_indexes':
                    improvements.append({
                        'priority': 'medium',
                        'category': 'database',
                        'issue': f"ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ {issue['count']}ê°œ",
                        'solution': 'ë¶ˆí•„ìš”í•œ ì¸ë±ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ INSERT/UPDATE ì„±ëŠ¥ í–¥ìƒ',
                        'code': 'DROP INDEX IF EXISTS index_name;'
                    })
                elif issue['type'] == 'slow_queries':
                    improvements.append({
                        'priority': 'high',
                        'category': 'performance',
                        'issue': f"ëŠë¦° ì¿¼ë¦¬ {issue['count']}ê°œ ë°œê²¬",
                        'solution': 'ì¿¼ë¦¬ ìµœì í™” ë° ì¸ë±ìŠ¤ ì¶”ê°€',
                        'code': 'CREATE INDEX idx_name ON table_name(column_name);'
                    })
                elif issue['type'] == 'vacuum_needed':
                    improvements.append({
                        'priority': 'medium',
                        'category': 'database',
                        'issue': f"VACUUMì´ í•„ìš”í•œ í…Œì´ë¸” {issue['count']}ê°œ",
                        'solution': 'VACUUM ì‹¤í–‰ìœ¼ë¡œ ë””ìŠ¤í¬ ê³µê°„ íšŒìˆ˜',
                        'code': 'VACUUM ANALYZE table_name;'
                    })
                    
        # ë³´ì•ˆ ê°œì„  í•„ìš”
        security = self.test_results.get('security', {})
        if not security.get('rate_limiting'):
            improvements.append({
                'priority': 'high',
                'category': 'security',
                'issue': 'Rate Limiting ë¯¸ì„¤ì •',
                'solution': 'DDoS ê³µê²© ë°©ì§€ë¥¼ ìœ„í•œ Rate Limiting ì„¤ì •',
                'code': self._get_rate_limiting_code()
            })
            
        if not security.get('secure_cookies'):
            improvements.append({
                'priority': 'high',
                'category': 'security',
                'issue': 'Secure Cookie ë¯¸ì„¤ì •',
                'solution': 'HTTPS í™˜ê²½ì—ì„œ ì¿ í‚¤ ë³´ì•ˆ ê°•í™”',
                'code': "SESSION_COOKIE_SECURE = True\nCSRF_COOKIE_SECURE = True"
            })
            
        # Redis ìºì‹œ ê°œì„ 
        redis_stats = self.test_results.get('performance', {}).get('redis', {})
        if redis_stats.get('hit_rate', 100) < 80:
            improvements.append({
                'priority': 'medium',
                'category': 'performance',
                'issue': f"ë‚®ì€ ìºì‹œ íˆíŠ¸ìœ¨ ({redis_stats.get('hit_rate', 0):.1f}%)",
                'solution': 'ìºì‹œ ì „ëµ ê°œì„  ë° TTL ì¡°ì •',
                'code': self._get_cache_optimization_code()
            })
            
        # API ì‘ë‹µ ì‹œê°„ ê°œì„ 
        metrics = self.test_results.get('performance', {}).get('metrics', {})
        for endpoint, times in metrics.items():
            if isinstance(times, dict) and times.get('avg_response_time', 0) > 1.0:
                improvements.append({
                    'priority': 'medium',
                    'category': 'performance',
                    'issue': f"{endpoint} ëŠë¦° ì‘ë‹µ ({times['avg_response_time']:.2f}ì´ˆ)",
                    'solution': 'ì¿¼ë¦¬ ìµœì í™” ë° ìºì‹± ì ìš©',
                    'code': self._get_query_optimization_code(endpoint)
                })
                
        self.test_results['improvements'] = improvements
        
        # ê°œì„ ì‚¬í•­ ì¶œë ¥
        if improvements:
            logger.info(f"\nğŸ“‹ ì´ {len(improvements)}ê°œì˜ ê°œì„ ì‚¬í•­ ë°œê²¬:")
            
            # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
            high_priority = [i for i in improvements if i['priority'] == 'high']
            medium_priority = [i for i in improvements if i['priority'] == 'medium']
            
            if high_priority:
                logger.info(f"\nğŸ”´ ë†’ì€ ìš°ì„ ìˆœìœ„ ({len(high_priority)}ê°œ):")
                for imp in high_priority:
                    logger.info(f"  - [{imp['category']}] {imp['issue']}")
                    logger.info(f"    â†’ {imp['solution']}")
                    
            if medium_priority:
                logger.info(f"\nğŸŸ¡ ì¤‘ê°„ ìš°ì„ ìˆœìœ„ ({len(medium_priority)}ê°œ):")
                for imp in medium_priority:
                    logger.info(f"  - [{imp['category']}] {imp['issue']}")
                    logger.info(f"    â†’ {imp['solution']}")
        else:
            logger.info("âœ… ì‹œìŠ¤í…œ ìƒíƒœ ì–‘í˜¸ - íŠ¹ë³„í•œ ê°œì„ ì‚¬í•­ ì—†ìŒ")
            
    def _get_rate_limiting_code(self):
        """Rate Limiting ì„¤ì • ì½”ë“œ"""
        return '''# config/rate_limit_middleware.py
from django.core.cache import cache
from django.http import JsonResponse
import time

class RateLimitMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response
        
    def __call__(self, request):
        # IP ê¸°ë°˜ rate limiting
        ip = self.get_client_ip(request)
        cache_key = f"rate_limit_{ip}"
        
        # 1ë¶„ì— 60íšŒ ì œí•œ
        requests = cache.get(cache_key, [])
        now = time.time()
        requests = [r for r in requests if r > now - 60]
        
        if len(requests) >= 60:
            return JsonResponse({"error": "Rate limit exceeded"}, status=429)
            
        requests.append(now)
        cache.set(cache_key, requests, 60)
        
        return self.get_response(request)
        
    def get_client_ip(self, request):
        x_forwarded_for = request.META.get(\'HTTP_X_FORWARDED_FOR\')
        if x_forwarded_for:
            ip = x_forwarded_for.split(\',\')[0]
        else:
            ip = request.META.get(\'REMOTE_ADDR\')
        return ip'''
        
    def _get_cache_optimization_code(self):
        """ìºì‹œ ìµœì í™” ì½”ë“œ"""
        return '''# ìºì‹œ ë°ì½”ë ˆì´í„° ì‚¬ìš©
from django.views.decorators.cache import cache_page
from django.core.cache import cache

# ë·° ë ˆë²¨ ìºì‹± (5ë¶„)
@cache_page(60 * 5)
def project_list(request):
    # ...
    
# ì¿¼ë¦¬ì…‹ ìºì‹±
def get_projects(user_id):
    cache_key = f"user_projects_{user_id}"
    projects = cache.get(cache_key)
    
    if projects is None:
        projects = Project.objects.filter(
            user_id=user_id
        ).select_related(\'user\').prefetch_related(\'members\')
        cache.set(cache_key, projects, 300)  # 5ë¶„ ìºì‹±
        
    return projects'''
        
    def _get_query_optimization_code(self, endpoint):
        """ì¿¼ë¦¬ ìµœì í™” ì½”ë“œ"""
        if 'projects' in endpoint:
            return '''# projects/views.py
from django.db.models import Prefetch

def get_queryset(self):
    return Project.objects.select_related(
        \'user\',
        \'basic_plan\'
    ).prefetch_related(
        \'members\',
        \'schedule_set\',
        Prefetch(
            \'files\',
            queryset=File.objects.filter(is_deleted=False)
        )
    ).annotate(
        member_count=Count(\'members\')
    )'''
        elif 'feedbacks' in endpoint:
            return '''# feedbacks/views.py
from django.db.models import Prefetch

def get_queryset(self):
    return Feedback.objects.select_related(
        \'user\',
        \'project\'
    ).prefetch_related(
        Prefetch(
            \'comments\',
            queryset=FeedbackComment.objects.select_related(\'user\')
        )
    ).only(
        \'id\', \'title\', \'video_url\', \'created_at\',
        \'user__name\', \'project__title\'
    )'''
        else:
            return '# í•´ë‹¹ ì—”ë“œí¬ì¸íŠ¸ì— ë§ëŠ” ì¿¼ë¦¬ ìµœì í™” í•„ìš”'
            
    def save_results(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        filename = f"backend_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
            
        logger.info(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ìš”ì•½ í†µê³„
        total_issues = len(self.test_results.get('improvements', []))
        high_priority = len([i for i in self.test_results.get('improvements', []) if i['priority'] == 'high'])
        
        logger.info("\n" + "="*60)
        logger.info("í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½")
        logger.info("="*60)
        logger.info(f"ì´ ë°œê²¬ëœ ì´ìŠˆ: {total_issues}ê°œ")
        logger.info(f"ë†’ì€ ìš°ì„ ìˆœìœ„ ì´ìŠˆ: {high_priority}ê°œ")
        
        if self.test_results.get('server_status', {}).get('status') == 'running':
            logger.info("âœ… ì„œë²„ ìƒíƒœ: ì •ìƒ")
        else:
            logger.info("âŒ ì„œë²„ ìƒíƒœ: ì´ìƒ")
            
        if self.test_results.get('database', {}).get('status') == 'connected':
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤: ì •ìƒ")
        else:
            logger.info("âŒ ë°ì´í„°ë² ì´ìŠ¤: ì´ìƒ")
            
        security_score = sum(1 for v in self.test_results.get('security', {}).values() if v)
        logger.info(f"ğŸ”’ ë³´ì•ˆ ì ìˆ˜: {security_score}/6")
        
        
if __name__ == '__main__':
    validator = BackendSystemValidator()
    validator.run_all_tests()